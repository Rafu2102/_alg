"""
ğŸš€ Snake AI V10.0 "Phoenix" - è¨“ç·´è…³æœ¬
========================================

é€™å€‹æª”æ¡ˆè² è²¬è¨“ç·´ AI æ¨¡å‹ï¼Œä½¿ç”¨ PPOï¼ˆProximal Policy Optimizationï¼‰æ¼”ç®—æ³•ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. è¨­å®šè¨“ç·´è¶…åƒæ•¸
2. å»ºç«‹ä¸¦è¡Œç’°å¢ƒï¼ˆ32 å€‹è²ªåƒè›‡åŒæ™‚è¨“ç·´ï¼‰
3. èª²ç¨‹å­¸ç¿’ï¼ˆCurriculum Learningï¼‰ï¼šåˆ†éšæ®µå¢åŠ é›£åº¦
4. è‡ªå‹•å„²å­˜æ¨¡å‹å’Œæ¢å¾©è¨“ç·´

æŠ€è¡“ç‰¹é»ï¼š
- PPO æ¼”ç®—æ³•ï¼šç©©å®šä¸”é«˜æ•ˆçš„å¼·åŒ–å­¸ç¿’æ–¹æ³•
- èª²ç¨‹å­¸ç¿’ï¼šå¾ç°¡å–®åˆ°å›°é›£ï¼Œå¾ªåºæ¼¸é€²
- TF32 åŠ é€Ÿï¼šåˆ©ç”¨ RTX 40 ç³»åˆ—çš„ç¡¬é«”åŠ é€Ÿ
- ä¸¦è¡Œè¨“ç·´ï¼š32 å€‹ç’°å¢ƒåŒæ™‚é‹è¡Œï¼Œå¤§å¹…æå‡æ•ˆç‡
"""

import os
import sys
import multiprocessing

# ==================== ç¡¬é«”å„ªåŒ–è¨­å®š ====================
# é€™äº›ç’°å¢ƒè®Šæ•¸ç”¨æ–¼å„ªåŒ–å¤šåŸ·è¡Œç·’æ•ˆèƒ½

# é™åˆ¶å„ç¨®æ•¸å­¸åº«åªä½¿ç”¨ 1 å€‹åŸ·è¡Œç·’
# å› ç‚ºæˆ‘å€‘ç”¨å¤šé€²ç¨‹ï¼ˆ32 å€‹ç’°å¢ƒï¼‰ï¼Œä¸éœ€è¦æ¯å€‹é€²ç¨‹å†é–‹å¤šåŸ·è¡Œç·’
os.environ["OMP_NUM_THREADS"] = "1"          # OpenMP
os.environ["MKL_NUM_THREADS"] = "1"          # Intel MKL
os.environ["NUMEXPR_NUM_THREADS"] = "1"      # NumExpr
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # Tokenizers

# Windows å¤šé€²ç¨‹ä¿®å¾©
# Windows éœ€è¦ä½¿ç”¨ 'spawn' æ–¹å¼å‰µå»ºå­é€²ç¨‹
if sys.platform == 'win32':
    multiprocessing.set_start_method('spawn', force=True)

import time
from datetime import datetime, timedelta
import numpy as np
import torch

# ==================== GPU å„ªåŒ–è¨­å®š ====================
# TF32 æ˜¯ NVIDIA Ampere æ¶æ§‹ï¼ˆRTX 30/40 ç³»åˆ—ï¼‰çš„åŠ é€ŸåŠŸèƒ½
# å¯ä»¥åœ¨å¹¾ä¹ä¸æå¤±ç²¾åº¦çš„æƒ…æ³ä¸‹æå‡è¨“ç·´é€Ÿåº¦

torch.backends.cudnn.benchmark = True          # cuDNN è‡ªå‹•å°‹æ‰¾æœ€ä½³å·ç©æ¼”ç®—æ³•
torch.backends.cuda.matmul.allow_tf32 = True   # å…è¨± TF32 çŸ©é™£ä¹˜æ³•
torch.backends.cudnn.allow_tf32 = True         # å…è¨± TF32 å·ç©
torch.set_float32_matmul_precision('high')     # é«˜ç²¾åº¦çŸ©é™£é‹ç®—
torch.set_num_threads(1)                       # PyTorch åªç”¨ 1 å€‹åŸ·è¡Œç·’

# åŒ¯å…¥å¼·åŒ–å­¸ç¿’ç›¸é—œå¥—ä»¶
from sb3_contrib import MaskablePPO           # æ”¯æ´å‹•ä½œé®ç½©çš„ PPO
from sb3_contrib.common.wrappers import ActionMasker  # å‹•ä½œé®ç½©åŒ…è£å™¨
from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor, VecNormalize
from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback
from stable_baselines3.common.utils import set_random_seed, get_schedule_fn

# åŒ¯å…¥æˆ‘å€‘çš„éŠæˆ²ç’°å¢ƒ
from snake_env_v10 import SnakeEnvV10

# ==================== PPO è¶…åƒæ•¸è¨­å®š ====================
# é€™äº›åƒæ•¸æ§åˆ¶ AI çš„å­¸ç¿’è¡Œç‚º

N_ENVS = 32           # ä¸¦è¡Œç’°å¢ƒæ•¸é‡ï¼ˆåŒæ™‚ç© 32 å±€éŠæˆ²ï¼‰
N_STEPS = 4096        # æ¯æ¬¡æ”¶é›†å¤šå°‘æ­¥ç¶“é©—å†å­¸ç¿’
BATCH_SIZE = 16384    # æ¯æ‰¹æ¬¡ç”¨å¤šå°‘æ¨£æœ¬è¨“ç·´
GAMMA = 0.999         # æŠ˜æ‰£å› å­ï¼šè¶Šæ¥è¿‘ 1 è¶Šé‡è¦–é•·æœŸçå‹µ
GAE_LAMBDA = 0.95     # GAEï¼ˆå»£ç¾©å„ªå‹¢ä¼°è¨ˆï¼‰çš„ Î» åƒæ•¸
VF_COEF = 0.5         # Value Function æå¤±å‡½æ•¸çš„æ¬Šé‡
CLIP_RANGE = 0.2      # PPO è£å‰ªç¯„åœï¼Œé˜²æ­¢ç­–ç•¥æ›´æ–°å¤ªå¤§
N_EPOCHS = 10         # æ¯æ‰¹è³‡æ–™å­¸ç¿’å¹¾æ¬¡
MAX_GRAD_NORM = 0.5   # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

"""
è¶…åƒæ•¸è§£é‡‹ï¼š

N_ENVS = 32ï¼š
- åŒæ™‚é‹è¡Œ 32 å€‹è²ªåƒè›‡éŠæˆ²
- æ¯æ­¥æ”¶é›† 32 ä»½ç¶“é©—ï¼Œå¤§å¹…æå‡è³‡æ–™æ•ˆç‡

N_STEPS = 4096ï¼š
- æ¯å€‹ç’°å¢ƒè·‘ 4096 æ­¥å¾Œæ‰æ›´æ–°æ¨¡å‹
- æ›´é•·çš„è»Œè·¡ = æ›´æº–ç¢ºçš„å„ªå‹¢ä¼°è¨ˆ

GAMMA = 0.999ï¼š
- æ¥è¿‘ 1 ä»£è¡¨éå¸¸é‡è¦–æœªä¾†çå‹µ
- è²ªåƒè›‡éœ€è¦é•·é è¦åŠƒï¼Œæ‰€ä»¥ç”¨é«˜ gamma

N_EPOCHS = 10ï¼š
- åŒä¸€æ‰¹è³‡æ–™åè¦†å­¸ç¿’ 10 æ¬¡
- å¤ªé«˜æœƒéæ“¬åˆï¼Œå¤ªä½å­¸ä¸å¤ 
"""


def linear_schedule(lr_start, lr_end):
    """
    ç·šæ€§å­¸ç¿’ç‡æ’ç¨‹
    
    ä»€éº¼æ˜¯å­¸ç¿’ç‡æ’ç¨‹ï¼Ÿ
    - å­¸ç¿’ç‡æ§åˆ¶æ¯æ¬¡æ›´æ–°çš„ã€Œæ­¥ä¼å¤§å°ã€
    - è¨“ç·´åˆæœŸç”¨å¤§å­¸ç¿’ç‡ï¼Œå¿«é€Ÿé€²æ­¥
    - è¨“ç·´å¾ŒæœŸç”¨å°å­¸ç¿’ç‡ï¼Œç²¾ç´°èª¿æ•´
    
    é€™å€‹å‡½æ•¸è¿”å›ä¸€å€‹æ’ç¨‹å‡½æ•¸ï¼Œæ ¹æ“šè¨“ç·´é€²åº¦è¿”å›å°æ‡‰çš„å­¸ç¿’ç‡ã€‚
    
    åƒæ•¸ï¼š
    - lr_start: åˆå§‹å­¸ç¿’ç‡
    - lr_end: çµæŸå­¸ç¿’ç‡
    
    è¿”å›ï¼š
    - schedule: æ’ç¨‹å‡½æ•¸ï¼Œæ¥å— progress_remainingï¼ˆ1.0 â†’ 0.0ï¼‰
    """
    def schedule(progress_remaining):
        # progress_remaining: 1.0ï¼ˆé–‹å§‹ï¼‰â†’ 0.0ï¼ˆçµæŸï¼‰
        # ç·šæ€§æ’å€¼ï¼šlr = lr_end + (lr_start - lr_end) * progress
        return lr_end + (lr_start - lr_end) * progress_remaining
    return schedule


# å…¨åŸŸéš¨æ©Ÿç¨®å­ï¼ˆç”¨æ–¼å¯é‡ç¾æ€§ï¼‰
BASE_SEED = 12345

# ==================== èª²ç¨‹å­¸ç¿’è¨­è¨ˆ ====================
# èª²ç¨‹å­¸ç¿’ (Curriculum Learning) æ˜¯ä¸€ç¨®è¨“ç·´ç­–ç•¥ï¼š
# å¾ç°¡å–®çš„ä»»å‹™é–‹å§‹ï¼Œé€æ¼¸å¢åŠ é›£åº¦

STAGES = [
    # ==================== Stage A: åŸºç¤å¥ å®šæœŸ ====================
    # ç›®æ¨™ï¼šè®“ AI å­¸æœƒåŸºæœ¬æŠ€èƒ½ï¼ˆåƒé£Ÿç‰©ã€é¿å…æ’ç‰†ï¼‰
    {
        "name": "v10_stage_a",        # éšæ®µåç¨±
        "grid_size": 20,              # ç¶²æ ¼å¤§å°
        "steps": 30_000_000,          # è¨“ç·´æ­¥æ•¸ï¼ˆ3000 è¬ï¼‰
        "lr_start": 3e-4,             # åˆå§‹å­¸ç¿’ç‡ 0.0003
        "lr_end": 2e-4,               # çµæŸå­¸ç¿’ç‡ 0.0002
        "ent_coef": 0.03,             # ç†µä¿‚æ•¸ï¼ˆé¼“å‹µæ¢ç´¢ï¼‰
        "start_length": 3,            # ç¸½æ˜¯å¾é•·åº¦ 3 é–‹å§‹
        "endgame_prob": 0.0,          # ä¸åš Endgame è¨“ç·´
        "target_length": 200          # ç›®æ¨™é•·åº¦
    },
    
    # ==================== Stage B: å¹³æ»‘éæ¸¡æœŸ ====================
    # ç›®æ¨™ï¼šé–‹å§‹æ¥è§¸ä¸­ç›¤å±€é¢ï¼Œä½†ä¸è¦å¤ªæ¿€é€²
    {
        "name": "v10_stage_b",
        "grid_size": 20,
        "steps": 70_000_000,          # 7000 è¬æ­¥
        "lr_start": 2e-4,
        "lr_end": 1e-4,
        "ent_coef": 0.025,            # ç¨å¾®æé«˜æ¢ç´¢
        "start_length": 50,           # 20% æ©Ÿç‡å¾é•·åº¦ 50 é–‹å§‹
        "endgame_prob": 0.2,          # Endgame å‡ºç¾æ©Ÿç‡
        "target_length": 350
    },
    
    # ==================== Stage C: è¡åˆºæœŸ ====================
    # ç›®æ¨™ï¼šæŒ‘æˆ°æ»¿åˆ† 400
    {
        "name": "v10_final",
        "grid_size": 20,
        "steps": 100_000_000,         # 1 å„„æ­¥
        "lr_start": 1.5e-4,
        "lr_end": 5e-5,
        "ent_coef": 0.02,
        "start_length": 100,          # 25% æ©Ÿç‡å¾é•·åº¦ 100 é–‹å§‹
        "endgame_prob": 0.25,
        "target_length": 400
    }
]

"""
èª²ç¨‹å­¸ç¿’çš„é‡è¦æ€§ï¼š

ç‚ºä»€éº¼ä¸èƒ½ä¸€é–‹å§‹å°±ç·´å›°é›£ä»»å‹™ï¼Ÿ
- AI ä¸€é–‹å§‹ä»€éº¼éƒ½ä¸æœƒ
- å¦‚æœç›´æ¥çµ¦å›°é›£ä»»å‹™ï¼Œæœƒå­¸ä¸åˆ°æ±è¥¿
- å¾ªåºæ¼¸é€²æ‰èƒ½ç©©å®šé€²æ­¥

æ¯å€‹éšæ®µçš„è¨­è¨ˆé‚è¼¯ï¼š
- Stage Aï¼šæ‰“å¥½åŸºç¤ï¼Œå­¸æœƒç”Ÿå­˜
- Stage Bï¼šé–‹å§‹æ¥è§¸ä¸­ç›¤ï¼Œä½† 80% æ™‚é–“è¤‡ç¿’åŸºç¤
- Stage Cï¼šæŒ‘æˆ° Endgameï¼Œè¡åˆºæ»¿åˆ†
"""

# ç¥ç¶“ç¶²è·¯æ¶æ§‹
POLICY_KWARGS = dict(
    net_arch=[dict(pi=[256, 256], vf=[256, 256])]
)
"""
ç¥ç¶“ç¶²è·¯æ¶æ§‹èªªæ˜ï¼š

pi=[256, 256]ï¼šPolicy ç¶²è·¯
- å…©å±¤å…¨é€£æ¥å±¤ï¼Œæ¯å±¤ 256 å€‹ç¥ç¶“å…ƒ
- è¼¸å…¥ï¼š26 ç¶­è§€å¯Ÿå‘é‡
- è¼¸å‡ºï¼š4 å€‹å‹•ä½œçš„æ©Ÿç‡åˆ†ä½ˆ

vf=[256, 256]ï¼šValue ç¶²è·¯
- åŒæ¨£æ˜¯å…©å±¤ 256 ç¥ç¶“å…ƒ
- è¼¸å‡ºï¼šç•¶å‰ç‹€æ…‹çš„åƒ¹å€¼ä¼°è¨ˆ

ç‚ºä»€éº¼ç”¨ 256ï¼Ÿ
- 26 ç¶­è¼¸å…¥ä¸éœ€è¦å¤ªå¤§çš„ç¶²è·¯
- V9.0 ç”¨ 512 åè€Œæ•ˆæœæ›´å·®ï¼ˆéåº¦è¨­è¨ˆï¼‰
"""


# ==================== çµ‚ç«¯æ©Ÿé¡è‰²ï¼ˆç¾åŒ–è¼¸å‡ºï¼‰====================
class C:
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BOLD = '\033[1m'
    DIM = '\033[2m'
    END = '\033[0m'
    MAGENTA = '\033[95m'

def clear():
    """æ¸…é™¤çµ‚ç«¯æ©Ÿç•«é¢"""
    os.system('cls' if os.name == 'nt' else 'clear')

def mask_fn(env):
    """å‹•ä½œé®ç½©å‡½æ•¸ï¼šè¿”å›ç•¶å‰å¯ç”¨çš„å‹•ä½œ"""
    return env.action_masks()

def make_env(grid_size, rank, seed=0, start_length=3, endgame_prob=0.0):
    """
    ç’°å¢ƒå·¥å» å‡½æ•¸
    
    é€™å€‹å‡½æ•¸å‰µå»ºä¸€å€‹è²ªåƒè›‡ç’°å¢ƒã€‚
    SubprocVecEnv æœƒèª¿ç”¨é€™å€‹å‡½æ•¸ N_ENVS æ¬¡ï¼Œå‰µå»ºå¤šå€‹ç’°å¢ƒã€‚
    
    åƒæ•¸ï¼š
    - grid_size: ç¶²æ ¼å¤§å°
    - rank: ç’°å¢ƒç·¨è™Ÿï¼ˆ0 åˆ° N_ENVS-1ï¼‰
    - seed: éš¨æ©Ÿç¨®å­
    - start_length: èª²ç¨‹å­¸ç¿’çš„èµ·å§‹é•·åº¦
    - endgame_prob: Endgame å‡ºç¾æ©Ÿç‡
    
    è¿”å›ï¼š
    - _init: åˆå§‹åŒ–å‡½æ•¸
    """
    def _init():
        # è¨­å®šé€™å€‹ç’°å¢ƒçš„éš¨æ©Ÿç¨®å­ï¼ˆæ¯å€‹ç’°å¢ƒä¸åŒï¼‰
        set_random_seed(seed + rank)
        import random as py_random
        py_random.seed(seed + rank)
        np.random.seed(seed + rank)
        
        # å‰µå»ºéŠæˆ²ç’°å¢ƒ
        env = SnakeEnvV10(
            grid_size=grid_size,
            default_start_length=start_length,
            endgame_prob=endgame_prob
        )
        # åŒ…è£å‹•ä½œé®ç½©
        env = ActionMasker(env, mask_fn)
        return env
    return _init


class V10ProgressCallback(BaseCallback):
    """
    V10.0 è¨“ç·´é€²åº¦å›å‘¼
    
    é€™å€‹é¡åˆ¥ç”¨æ–¼ï¼š
    1. é¡¯ç¤ºè¨“ç·´é€²åº¦ï¼ˆæ¼‚äº®çš„çµ‚ç«¯æ©Ÿä»‹é¢ï¼‰
    2. è¨˜éŒ„çµ±è¨ˆè³‡æ–™ï¼ˆæœ€ä½³é•·åº¦ã€å¹³å‡é•·åº¦ç­‰ï¼‰
    3. è‡ªå‹•å„²å­˜é‡Œç¨‹ç¢‘æ¨¡å‹ï¼ˆé”åˆ° 50, 100, 200... æ™‚ï¼‰
    """
    
    def __init__(self, stage_name, total_steps, target_length=400, ent_coef=0.01, verbose=0):
        super().__init__(verbose)
        self.stage_name = stage_name
        self.total_steps = total_steps
        self.target_length = target_length
        self.ent_coef = ent_coef
        
        # çµ±è¨ˆè³‡æ–™
        self.episode_lengths = []  # æ¯å±€éŠæˆ²çš„è›‡é•·åº¦
        self.fallback_counts = []  # Fallback è§¸ç™¼æ¬¡æ•¸
        self.best_length = 3       # æ­·å²æœ€ä½³é•·åº¦
        self.best_avg = 0          # æ­·å²æœ€ä½³å¹³å‡
        self.generation = 0        # ç¸½éŠæˆ²å±€æ•¸
        self.start_time = time.time()
        self.last_display = 0
        
        # é‡Œç¨‹ç¢‘ï¼ˆé”åˆ°é€™äº›é•·åº¦æ™‚å„²å­˜æ¨¡å‹ï¼‰
        self.milestones = [50, 100, 150, 200, 250, 300, 350, 375, 400]
        self.achieved = set()  # å·²é”æˆçš„é‡Œç¨‹ç¢‘
        
    def _on_step(self):
        """
        æ¯æ­¥è¨“ç·´éƒ½æœƒèª¿ç”¨é€™å€‹æ–¹æ³•
        
        æ³¨æ„ï¼šåªåœ¨ episode çµæŸæ™‚è¨˜éŒ„é•·åº¦ï¼ˆdone=Trueï¼‰
        """
        dones = self.locals.get('dones', [])
        infos = self.locals.get('infos', [])
        
        for done, info in zip(dones, infos):
            if done:  # åªåœ¨éŠæˆ²çµæŸæ™‚è¨˜éŒ„
                length = info.get('length', 0)
                if length > 0:
                    self.episode_lengths.append(length)
                    self.generation += 1
                    
                    if 'fallback_count' in info:
                        self.fallback_counts.append(info['fallback_count'])
                    
                    # æª¢æŸ¥æ–°ç´€éŒ„
                    if length > self.best_length:
                        self.best_length = length
                        
                        # æª¢æŸ¥é‡Œç¨‹ç¢‘
                        for m in self.milestones:
                            if length >= m and m not in self.achieved:
                                self.achieved.add(m)
                                self.model.save(f"checkpoints/{self.stage_name}_milestone_{m}")
                                print(f"\n{C.GREEN}ğŸ† é‡Œç¨‹ç¢‘é”æˆ: {m}! å·²å„²å­˜!{C.END}\n")
        
        # è¨˜éŒ„åˆ° TensorBoard
        if len(self.fallback_counts) > 0:
            fallback_mean = np.mean(self.fallback_counts[-100:])
            self.logger.record("custom/fallback_mean", fallback_mean)
        
        if len(self.episode_lengths) > 0:
            recent_avg = np.mean(self.episode_lengths[-500:])
            self.logger.record("custom/length_avg_500", recent_avg)
            
            # å„²å­˜æœ€ä½³å¹³å‡æ¨¡å‹
            if recent_avg > self.best_avg and len(self.episode_lengths) >= 500:
                self.best_avg = recent_avg
                self.model.save(f"checkpoints/{self.stage_name}_best_avg_{int(recent_avg)}")
                self.logger.record("custom/best_avg", self.best_avg)
            
            # è¨ˆç®—é”æ¨™ç™¾åˆ†æ¯”
            recent = self.episode_lengths[-500:]
            pct_target = sum(1 for l in recent if l >= self.target_length) / len(recent) * 100
            self.logger.record("custom/pct_target", pct_target)
        
        # å®šæœŸæ›´æ–°é¡¯ç¤º
        now = time.time()
        if now - self.last_display >= 1.0:
            self._display()
            self.last_display = now
        
        return True
    
    def _display(self):
        """é¡¯ç¤ºæ¼‚äº®çš„è¨“ç·´é€²åº¦ä»‹é¢"""
        clear()
        
        elapsed = time.time() - self.start_time
        steps = self.num_timesteps
        progress = min(100, steps / self.total_steps * 100)
        
        recent = self.episode_lengths[-500:] if self.episode_lengths else [3]
        avg = np.mean(recent)
        max_r = max(recent) if recent else 3
        p90 = np.percentile(recent, 90) if len(recent) >= 10 else max_r
        
        sps = steps / elapsed if elapsed > 0 else 0
        eta = (self.total_steps - steps) / sps if sps > 0 else 0
        
        # GPU è³‡è¨Š
        gpu = "CPU"
        if torch.cuda.is_available():
            name = torch.cuda.get_device_name(0)[:20]
            mem = torch.cuda.memory_allocated() / 1024**3
            gpu = f"{name} | {mem:.1f}GB"
        
        # é€²åº¦æ¢
        bar_len = 50
        filled = int(bar_len * progress / 100)
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_len - filled)
        
        # é‡Œç¨‹ç¢‘é¡¯ç¤º
        ms = ""
        for m in self.milestones:
            if m in self.achieved:
                ms += f"{C.GREEN}[{m}]{C.END} "
            elif m <= self.best_length:
                ms += f"{C.YELLOW}~{m}{C.END} "
            else:
                ms += f"{C.DIM}{m}{C.END} "
        
        print(f"""
{C.CYAN}â•”{'â•'*78}â•—{C.END}
{C.CYAN}â•‘{C.END}      {C.BOLD}{C.MAGENTA}SNAKE AI V10.0 "PHOENIX" - {self.stage_name.upper()}{C.END}              {C.CYAN}â•‘{C.END}
{C.CYAN}â• {'â•'*78}â•£{C.END}
{C.CYAN}â•‘{C.END}  {C.DIM}GPU: {gpu:<60}{C.END}  {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  {C.DIM}Net: 256Ã—256 | Envs: {N_ENVS} | Batch: {BATCH_SIZE} | ent: {self.ent_coef}{C.END}      {C.CYAN}â•‘{C.END}
{C.CYAN}â• {'â”€'*78}â•£{C.END}
{C.CYAN}â•‘{C.END}                                                                              {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  {C.YELLOW}ğŸ“Š è¨“ç·´ç‹€æ…‹{C.END}                                                       {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  Generation:     {C.GREEN}{self.generation:>15,}{C.END}                                          {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  Total Steps:    {C.GREEN}{steps:>15,}{C.END}                                          {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  Speed:          {C.GREEN}{sps:>15,.0f}{C.END} steps/s                               {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}                                                                              {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  {C.YELLOW}ğŸ¯ è¡¨ç¾ (ç›®æ¨™: {self.target_length}){C.END}                                       {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  Best Length:    {C.GREEN}{self.best_length:>15}{C.END}                                          {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  Avg (500):      {C.GREEN}{avg:>15.1f}{C.END}                                          {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  Max Recent:     {C.GREEN}{max_r:>15}{C.END}                                          {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  P90 (Top 10%):  {C.GREEN}{p90:>15.1f}{C.END}                                          {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}                                                                              {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  {C.YELLOW}ğŸ† é‡Œç¨‹ç¢‘{C.END}                                                            {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  {ms:<76}{C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}                                                                              {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  {C.YELLOW}â±ï¸ æ™‚é–“{C.END}                                                                  {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  Elapsed:        {str(timedelta(seconds=int(elapsed))):>15}                                  {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  ETA:            {str(timedelta(seconds=int(eta))):>15}                                  {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}                                                                              {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  {C.YELLOW}ğŸ“ˆ é€²åº¦{C.END}                                                              {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}  [{bar}] {progress:>5.1f}%        {C.CYAN}â•‘{C.END}
{C.CYAN}â•‘{C.END}                                                                              {C.CYAN}â•‘{C.END}
{C.CYAN}â• {'â”€'*78}â•£{C.END}
{C.CYAN}â•‘{C.END}  {C.RED}æŒ‰ Ctrl+C æš«åœä¸¦å„²å­˜{C.END}                                          {C.CYAN}â•‘{C.END}
{C.CYAN}â•š{'â•'*78}â•{C.END}
""")


def main():
    """
    ä¸»è¨“ç·´æµç¨‹
    
    æµç¨‹ï¼š
    1. åˆå§‹åŒ– GPU å’Œéš¨æ©Ÿç¨®å­
    2. å°æ¯å€‹ Stageï¼š
       a. å‰µå»ºä¸¦è¡Œç’°å¢ƒ
       b. è¼‰å…¥æˆ–å‰µå»ºæ¨¡å‹
       c. è¨“ç·´æŒ‡å®šæ­¥æ•¸
       d. å„²å­˜æ¨¡å‹
    3. å®Œæˆ
    """
    print(f"{C.MAGENTA}{'='*60}{C.END}")
    print(f"{C.MAGENTA}  SNAKE AI V10.0 \"PHOENIX\" - èª²ç¨‹å­¸ç¿’è¨“ç·´{C.END}")
    print(f"{C.MAGENTA}{'='*60}{C.END}")
    print(f"{C.CYAN}  26ç¶­obs | 256Ã—256 MLP | TF32 | 32 envs{C.END}")
    
    # ==================== è¨­å®šå…¨åŸŸéš¨æ©Ÿç¨®å­ ====================
    print(f"{C.DIM}  è¨­å®šå…¨åŸŸç¨®å­: {BASE_SEED}{C.END}")
    set_random_seed(BASE_SEED)
    import random as py_random
    py_random.seed(BASE_SEED)
    np.random.seed(BASE_SEED)
    torch.manual_seed(BASE_SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(BASE_SEED)
    
    # GPU æª¢æŸ¥
    if torch.cuda.is_available():
        print(f"{C.GREEN}[âœ“] GPU: {torch.cuda.get_device_name(0)}{C.END}")
        device = "cuda"
        torch.backends.cudnn.benchmark = True
        torch.set_num_threads(1)
    else:
        print(f"{C.YELLOW}[!] æ²’æœ‰ GPUï¼Œä½¿ç”¨ CPU{C.END}")
        device = "cpu"
    
    # å‰µå»º checkpoints ç›®éŒ„
    os.makedirs("checkpoints", exist_ok=True)
    
    # ==================== Checkpoint è‡ªå‹•æ¢å¾© ====================
    def find_latest_checkpoint(stage_name):
        """å°‹æ‰¾æŸå€‹éšæ®µçš„æœ€æ–° checkpoint"""
        import glob
        import re
        
        pattern = f"checkpoints/{stage_name}_*_steps.zip"
        checkpoints = glob.glob(pattern)
        
        if not checkpoints:
            return None, 0
        
        max_steps = 0
        latest_path = None
        for cp in checkpoints:
            match = re.search(r'_(\d+)_steps\.zip$', cp)
            if match:
                steps = int(match.group(1))
                if steps > max_steps:
                    max_steps = steps
                    latest_path = cp.replace('.zip', '')
        
        return latest_path, max_steps
    
    def find_vecnorm_for_checkpoint(checkpoint_path):
        """å°‹æ‰¾å°æ‡‰çš„ VecNormalize æª”æ¡ˆ"""
        import os
        stage_name = os.path.basename(checkpoint_path).split('_')[0] + '_' + os.path.basename(checkpoint_path).split('_')[1] + '_' + os.path.basename(checkpoint_path).split('_')[2]
        vecnorm_path = f"checkpoints/{stage_name}_vecnorm.pkl"
        if os.path.exists(vecnorm_path):
            return vecnorm_path
        for stage in STAGES:
            stage_vecnorm = f"checkpoints/{stage['name']}_vecnorm.pkl"
            if os.path.exists(stage_vecnorm):
                return stage_vecnorm
        return None
    
    current_model_path = None
    current_vecnorm_path = None
    
    # ==================== ä¸»è¨“ç·´è¿´åœˆ ====================
    for i, stage in enumerate(STAGES):
        name = stage['name']
        grid_size = stage['grid_size']
        steps = stage['steps']
        lr_start = stage['lr_start']
        lr_end = stage['lr_end']
        ent_coef = stage['ent_coef']
        start_length = stage['start_length']
        endgame_prob = stage['endgame_prob']
        target_length = stage['target_length']
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ checkpoint å¯ä»¥æ¢å¾©
        checkpoint_path, steps_completed = find_latest_checkpoint(name)
        remaining_steps = max(0, steps - steps_completed)
        
        # è·³éå·²å®Œæˆçš„éšæ®µ
        if os.path.exists(f"checkpoints/{name}.zip") and os.path.exists(f"checkpoints/{name}_vecnorm.pkl"):
            print(f"\n{C.GREEN}[âœ“] éšæ®µ {name} å·²å®Œæˆï¼Œè·³é...{C.END}")
            current_model_path = f"checkpoints/{name}"
            current_vecnorm_path = f"checkpoints/{name}_vecnorm.pkl"
            continue
        
        # å¾ checkpoint æ¢å¾©
        if checkpoint_path and steps_completed > 0:
            print(f"\n{C.YELLOW}[!] æ‰¾åˆ° checkpoint: {checkpoint_path} ({steps_completed:,} æ­¥å·²å®Œæˆ){C.END}")
            print(f"{C.YELLOW}    ç¹¼çºŒè¨“ç·´ï¼Œå‰©é¤˜ {remaining_steps:,} æ­¥...{C.END}")
            current_model_path = checkpoint_path
            found_vecnorm = find_vecnorm_for_checkpoint(checkpoint_path)
            if found_vecnorm:
                current_vecnorm_path = found_vecnorm
        else:
            remaining_steps = steps
        
        print(f"\n{C.CYAN}{'='*60}{C.END}")
        print(f"{C.CYAN}ğŸš€ éšæ®µ {i+1}/3: {name.upper()}{C.END}")
        print(f"{C.CYAN}   ç¶²æ ¼: {grid_size}x{grid_size} | æ­¥æ•¸: {remaining_steps:,} | LR: {lr_start}â†’{lr_end}{C.END}")
        print(f"{C.CYAN}   Endgame: {endgame_prob*100:.0f}% | èµ·å§‹é•·åº¦: {start_length} | ent: {ent_coef}{C.END}")
        print(f"{C.CYAN}{'='*60}{C.END}")
        
        # ==================== å‰µå»ºä¸¦è¡Œç’°å¢ƒ ====================
        print(f"{C.DIM}  å‰µå»º {N_ENVS} å€‹ä¸¦è¡Œç’°å¢ƒ...{C.END}")
        env = SubprocVecEnv([
            make_env(grid_size, k, seed=BASE_SEED, 
                    start_length=start_length, endgame_prob=endgame_prob) 
            for k in range(N_ENVS)
        ])
        env = VecMonitor(env)  # ç›£æ§ç’°å¢ƒï¼ˆè¨˜éŒ„çµ±è¨ˆï¼‰
        
        # è¼‰å…¥æˆ–å‰µå»º VecNormalize
        if current_vecnorm_path and os.path.exists(current_vecnorm_path):
            print(f"{C.GREEN}[âœ“] è¼‰å…¥ VecNormalize: {current_vecnorm_path}{C.END}")
            env = VecNormalize.load(current_vecnorm_path, env)
            env.training = True
        else:
            print(f"{C.YELLOW}  å‰µå»ºæ–°çš„ VecNormalize...{C.END}")
            env = VecNormalize(env, norm_obs=True, norm_reward=True, 
                              clip_obs=10.0, clip_reward=100.0)
        
        # ==================== å‰µå»ºæˆ–è¼‰å…¥æ¨¡å‹ ====================
        lr_schedule = linear_schedule(lr_start, lr_end)
        
        if current_model_path is None:
            print(f"{C.YELLOW}  å‰µå»ºæ–°æ¨¡å‹ (256Ã—256)...{C.END}")
            model = MaskablePPO(
                "MlpPolicy",
                env,
                verbose=0,
                learning_rate=lr_schedule,
                batch_size=BATCH_SIZE,
                n_steps=N_STEPS,
                gamma=GAMMA,
                gae_lambda=GAE_LAMBDA,
                ent_coef=ent_coef,
                vf_coef=VF_COEF,
                clip_range=CLIP_RANGE,
                n_epochs=N_EPOCHS,
                max_grad_norm=MAX_GRAD_NORM,
                target_kl=0.03,  # é˜²æ­¢æ›´æ–°å¤ªæ¿€é€²
                policy_kwargs=POLICY_KWARGS,
                device=device,
                tensorboard_log="./snake_v10_logs/"
            )
        else:
            print(f"{C.GREEN}[âœ“] è¼‰å…¥æ¨¡å‹: {current_model_path}{C.END}")
            model = MaskablePPO.load(current_model_path, env=env, device=device)
            
            # æ›´æ–°å­¸ç¿’ç‡å’Œç†µä¿‚æ•¸
            model.lr_schedule = get_schedule_fn(lr_schedule)
            model.ent_coef = ent_coef
            print(f"{C.GREEN}  LR: {lr_start}â†’{lr_end} | ent_coef: {ent_coef}{C.END}")
        
        # ==================== è¨­å®šå›å‘¼å‡½æ•¸ ====================
        callbacks = [
            V10ProgressCallback(name, remaining_steps, 
                               target_length=target_length, ent_coef=ent_coef),
            CheckpointCallback(
                save_freq=2_000_000 // N_ENVS,  # æ¯ 200 è¬æ­¥å„²å­˜
                save_path="./checkpoints/",
                name_prefix=name
            )
        ]
        
        print(f"{C.GREEN}[âœ“] é–‹å§‹è¨“ç·´!{C.END}")
        time.sleep(1)
        
        # ==================== è¨“ç·´ ====================
        try:
            model.learn(
                total_timesteps=remaining_steps,
                callback=callbacks,
                progress_bar=False,
                reset_num_timesteps=(steps_completed == 0)
            )
        except KeyboardInterrupt:
            print(f"\n{C.YELLOW}æš«åœä¸­ï¼Œæ­£åœ¨å„²å­˜...{C.END}")
        
        # ==================== å„²å­˜ ====================
        save_path = f"checkpoints/{name}"
        vecnorm_path = f"checkpoints/{name}_vecnorm.pkl"
        
        model.save(save_path)
        env.save(vecnorm_path)
        
        current_model_path = save_path
        current_vecnorm_path = vecnorm_path
        
        env.close()
        
        print(f"{C.GREEN}[âœ“] éšæ®µ {i+1} å®Œæˆ! å·²å„²å­˜è‡³ {save_path}{C.END}")
        
        # æœ€çµ‚å„²å­˜ï¼ˆåŠ æ™‚é–“æˆ³ï¼‰
        if i == len(STAGES) - 1:
            ts = datetime.now().strftime('%Y%m%d_%H%M%S')
            model.save(f"checkpoints/v10_final_{ts}")
            print(f"{C.GREEN}[âœ“] æœ€çµ‚æ¨¡å‹å·²å„²å­˜: v10_final_{ts}.zip{C.END}")
    
    print(f"\n{C.MAGENTA}{'='*60}{C.END}")
    print(f"{C.MAGENTA}  V10.0 è¨“ç·´å®Œæˆ!{C.END}")
    print(f"{C.MAGENTA}{'='*60}{C.END}")
    print(f"\n{C.CYAN}åŸ·è¡Œ: python watch_v10.py{C.END}")


if __name__ == "__main__":
    multiprocessing.freeze_support()  # Windows å¤šé€²ç¨‹æ”¯æ´
    main()
